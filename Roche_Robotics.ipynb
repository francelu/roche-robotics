{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3eff37-2b07-41b2-9dd4-f449cad13574",
   "metadata": {},
   "source": [
    "# Roche Robotics project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e824111-afdb-428b-a1f4-a262b14913f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dde099-46eb-4643-9b94-ba50d8cadb00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Installation of ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9d90fb-131f-4cec-9669-0218aada4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a5fb6-8a2e-4118-af1f-07f2c8e65cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4aa50e-8d22-42e2-ba4a-e14ea70cfd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23557fb9-dda6-44a6-bf12-da87aeec3ce5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4252391-2c7e-4f08-b62a-0c104afb988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 100\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4bd17-df59-4094-a4bf-c059b2c89a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import habitat_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b31a98-068b-4c06-97d0-95b938a5137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e45344c-889a-4d27-8b8e-58cd2ca048c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import magnum as mn\n",
    "from habitat_sim.physics import ManagedRigidObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55865b24-9322-4062-94b4-1e1bc02cd582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from magnum import Vector3, Quaternion, Rad\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119d814b-5c46-46c5-8fe6-6f9fbec09c2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Object container class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea871e-d6e5-42ef-b9f1-8a435db4537d",
   "metadata": {},
   "source": [
    "This module provides helper classes to load, scale, and place static objects (walls, obstacles, and a target) in a Habitat-Sim scene. It manages object templates efficiently and allows for dynamic scene modification, including navmesh recomputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c26c3-10d3-4503-9a88-9f073bb62ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectTemplateContainer:\n",
    "    def __init__(self, obj_path: str, scale: List[float]) -> None:\n",
    "        self.obj_path: str = obj_path\n",
    "        self.scale: np.ndarray = np.array(scale, dtype=float)\n",
    "        self.handle = self._propose_handle(\n",
    "            obj_path=obj_path,\n",
    "            scale=scale  # use list\n",
    "        )\n",
    "\n",
    "    def _propose_handle(self, obj_path: str, scale: List[float]) -> str:\n",
    "        base, suffix = obj_path.split('.')\n",
    "        for s in scale:\n",
    "            base += '_' + self._smart_round(s)\n",
    "        return base + '.' + suffix\n",
    "\n",
    "    @staticmethod\n",
    "    def _smart_round(val: float, n_places: int = 3) -> str:\n",
    "        rounded = round(val, n_places)\n",
    "        if rounded == int(val):  # if val is an int\n",
    "            return str(int(val))  # return val as str\n",
    "        else:\n",
    "            return str(rounded).replace('.', '-')  # return rounded number as str\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        return other.obj_path == self.obj_path \\\n",
    "            and np.allclose(other.scale, self.scale)\n",
    "\n",
    "    def eq(self, other_obj_path: str, other_scale: List[float]):\n",
    "        return other_obj_path == self.obj_path \\\n",
    "            and np.allclose(other_scale, self.scale)\n",
    "\n",
    "class RunTimeObjectManager:\n",
    "    def __init__(self, sim: habitat_sim.Simulator) -> None:\n",
    "        self._sim = sim\n",
    "        self._obj_templates_mgr = self._sim.get_object_template_manager()\n",
    "        self._rigid_obj_mgr = self._sim.get_rigid_object_manager()\n",
    "\n",
    "        self._navmesh_settings = habitat_sim.NavMeshSettings()\n",
    "        self._navmesh_settings.set_defaults()\n",
    "\n",
    "        self._raw_tpl_ctrs: List[ObjectTemplateContainer] = []\n",
    "        self._scaled_tpl_ctrs: List[ObjectTemplateContainer] = []\n",
    "\n",
    "        self._added_target: Optional[ManagedRigidObject] = None\n",
    "        self._added_walls: List[ManagedRigidObject] = []\n",
    "        self._added_obstacles: List[ManagedRigidObject] = []\n",
    "\n",
    "    def add_walls(self, walls_layout_cfg: List[Dict[str, Any]]) -> None:\n",
    "        for wall_cfg in walls_layout_cfg:\n",
    "            wall_obj = self._add_static_obj(\n",
    "                obj_path=wall_cfg['asset'],\n",
    "                position=wall_cfg['pos'],\n",
    "                rotation=mn.Quaternion.rotation(\n",
    "                    mn.Deg(wall_cfg['rot_amount']), wall_cfg['rot_vector']\n",
    "                ),\n",
    "                scale=wall_cfg['scale'],\n",
    "            )\n",
    "\n",
    "            self._added_walls.append(wall_obj)\n",
    "\n",
    "    def add_obstacles(self, obstacles_layout_cfg: List[Dict[str, Any]]) -> None:\n",
    "        for obstacle_cfg in obstacles_layout_cfg:\n",
    "            obstacle_obj = self._add_static_obj(\n",
    "                obj_path=obstacle_cfg['asset'],\n",
    "                position=obstacle_cfg['pos'],\n",
    "                rotation=mn.Quaternion.rotation(\n",
    "                    mn.Deg(obstacle_cfg['rot_amount']),\n",
    "                    obstacle_cfg['rot_vector']\n",
    "                ),\n",
    "                scale=obstacle_cfg['scale'],\n",
    "            )\n",
    "\n",
    "            self._added_obstacles.append(obstacle_obj)\n",
    "\n",
    "    def add_target(self, target_path: str, position: List[float]) -> None:\n",
    "\n",
    "        target_obj = self._add_static_obj(\n",
    "            obj_path=target_path,\n",
    "            position=position,\n",
    "            rotation=mn.Quaternion.rotation(\n",
    "                mn.Deg(0.0),\n",
    "                [0.0, 1.0, 0.0]\n",
    "            ),  # Goal in PointNav has no rotation\n",
    "            scale=[1.0, 1.0, 1.0],  # Maybe various scales?\n",
    "            # scale=[10.0, 10.0, 10.0],\n",
    "        )\n",
    "\n",
    "        self._added_target = target_obj\n",
    "\n",
    "    def _add_static_obj(\n",
    "        self, obj_path: str, scale: List[float],\n",
    "        position: List[float], rotation: mn.Quaternion.rotation\n",
    "    ) -> ManagedRigidObject:\n",
    "        has_raw_tpl, raw_tpl_ctr = self._has_raw_template(\n",
    "            obj_path=obj_path\n",
    "        )\n",
    "        if not has_raw_tpl:  # Load raw tpl\n",
    "            raw_obj_tpl_id = self._obj_templates_mgr.load_configs(\n",
    "                path=obj_path\n",
    "            )[0]\n",
    "            raw_obj_tpl = self._obj_templates_mgr.get_template_by_id(\n",
    "                template_id=raw_obj_tpl_id\n",
    "            )\n",
    "            raw_scale = raw_obj_tpl.scale\n",
    "            raw_tpl_ctr = ObjectTemplateContainer(\n",
    "                obj_path=obj_path,\n",
    "                scale=raw_scale\n",
    "            )\n",
    "            # Raw handle is from mgr\n",
    "            raw_tpl_ctr.handle = self._obj_templates_mgr.get_template_handle_by_id(\n",
    "                template_id=raw_obj_tpl_id\n",
    "            )\n",
    "            self._raw_tpl_ctrs.append(raw_tpl_ctr)\n",
    "        else:\n",
    "            raw_obj_tpl = self._obj_templates_mgr.get_template_by_handle(\n",
    "                handle=raw_tpl_ctr.handle\n",
    "            )\n",
    "\n",
    "        has_scaled_tpl, scaled_tpl_ctr = self._has_scaled_template(\n",
    "            obj_path=obj_path,\n",
    "            scale=scale\n",
    "        )\n",
    "        if not has_scaled_tpl:\n",
    "            scaled_tpl_ctr = ObjectTemplateContainer(\n",
    "                obj_path=obj_path,\n",
    "                scale=scale\n",
    "            )\n",
    "\n",
    "            raw_obj_tpl.scale = scale\n",
    "            self._obj_templates_mgr.register_template(\n",
    "                template=raw_obj_tpl,\n",
    "                specified_handle=scaled_tpl_ctr.handle\n",
    "            )\n",
    "            self._scaled_tpl_ctrs.append(scaled_tpl_ctr)\n",
    "\n",
    "        obj: ManagedRigidObject = \\\n",
    "            self._rigid_obj_mgr.add_object_by_template_handle(\n",
    "                object_lib_handle=scaled_tpl_ctr.handle\n",
    "            )\n",
    "\n",
    "        obj.translation = mn.Vector3(position)\n",
    "        obj.rotation = rotation\n",
    "        obj.motion_type = habitat_sim.physics.MotionType.STATIC\n",
    "\n",
    "        return obj\n",
    "\n",
    "    def _has_raw_template(\n",
    "        self, obj_path: str\n",
    "    ) -> Tuple[bool, Optional[ObjectTemplateContainer]]:\n",
    "        for tpl_ctr in self._raw_tpl_ctrs:\n",
    "            if tpl_ctr.obj_path == obj_path:\n",
    "                return True, tpl_ctr\n",
    "        return False, None\n",
    "\n",
    "    def _has_scaled_template(\n",
    "        self, obj_path: str, scale: List[float]\n",
    "    ):\n",
    "        for tpl_ctr in self._scaled_tpl_ctrs:\n",
    "            if tpl_ctr.eq(obj_path, scale):\n",
    "                return True, tpl_ctr\n",
    "        return False, None\n",
    "\n",
    "    def delete_added_target(self) -> None:\n",
    "        if self._added_target is None:\n",
    "            print(\"[DEBUG] No target to delete\")\n",
    "            return\n",
    "\n",
    "        \n",
    "        print(f\"[DEBUG] existing target to delete, id={self._added_target.object_id}\")\n",
    "        self._rigid_obj_mgr.remove_object_by_id(\n",
    "            object_id=self._added_target.object_id\n",
    "        )\n",
    "        self._added_target = None\n",
    "\n",
    "    def delete_added_walls(self) -> None:\n",
    "        if len(self._added_walls) <= 0:\n",
    "            return\n",
    "        while len(self._added_walls) > 0:\n",
    "            wall_to_rm = self._added_walls.pop()\n",
    "            self._rigid_obj_mgr.remove_object_by_id(\n",
    "                object_id=wall_to_rm.object_id\n",
    "            )\n",
    "\n",
    "    def delete_added_obstacles(self) -> None:\n",
    "        if len(self._added_obstacles) <= 0:\n",
    "            return\n",
    "        while len(self._added_obstacles) > 0:\n",
    "            obstacle_to_rm = self._added_obstacles.pop()\n",
    "            self._rigid_obj_mgr.remove_object_by_id(\n",
    "                object_id=obstacle_to_rm.object_id\n",
    "            )\n",
    "\n",
    "    def recompute_navmesh(self) -> None:\n",
    "        self._sim.recompute_navmesh(\n",
    "            pathfinder=self._sim.pathfinder,\n",
    "            navmesh_settings=self._navmesh_settings,\n",
    "            include_static_objects=True  # Take int account added objects\n",
    "        )\n",
    "\n",
    "    def refresh(self, sim: habitat_sim.Simulator) -> None:\n",
    "        if sim is not self._sim:  # new sim\n",
    "            self._sim = sim\n",
    "            self._obj_templates_mgr = self._sim.get_object_template_manager()\n",
    "\n",
    "            # Clear\n",
    "            self._raw_tpl_ctrs: List[ObjectTemplateContainer] = []\n",
    "            self._scaled_tpl_ctrs: List[ObjectTemplateContainer] = []\n",
    "\n",
    "        if self._sim.get_rigid_object_manager() is not self._rigid_obj_mgr:  # sim reconfigured\n",
    "            self._rigid_obj_mgr = self._sim.get_rigid_object_manager()\n",
    "\n",
    "            # Clear\n",
    "            self._added_target: Optional[ManagedRigidObject] = None\n",
    "            self._added_walls: List[ManagedRigidObject] = []\n",
    "            self._added_obstacles: List[ManagedRigidObject] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6e75f8-7e7e-4582-bc0e-994f172ea164",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Simulator creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3746a9ce-d16a-4e64-b9ac-2d11bcc976a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sensor and Agent configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b39d16a-d611-4303-9f84-25b19e9dc911",
   "metadata": {},
   "source": [
    "This section defines the agent's RGB and depth sensors.  \r\n",
    "To train or test models using specific modalities (e.g., RGB-only or Depth-only), you can **comment or uncomment** the corresponding sensor specs below.  \r\n",
    "\r\n",
    "Adjust the `sensor_specs` list to control which sensors are active during simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823bd99e-fd29-4408-a15c-b749dd5a0398",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_settings = {\n",
    "    \"height\": 256, \"width\": 256,  # Spatial resolution of observations\n",
    "    \"sensor_height\": 0,  # Height of sensors in meters, relative to the agent\n",
    "}\n",
    "\n",
    "# Create a RGB sensor configuration\n",
    "rgb_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "rgb_sensor_spec.uuid = \"color_sensor\"\n",
    "rgb_sensor_spec.sensor_type = habitat_sim.SensorType.COLOR\n",
    "rgb_sensor_spec.resolution = [sensor_settings[\"height\"], sensor_settings[\"width\"]]\n",
    "rgb_sensor_spec.position = [0.0, sensor_settings[\"sensor_height\"], 0.0]\n",
    "rgb_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "\n",
    "#Create a depth sensor configuration\n",
    "depth_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "depth_sensor_spec.uuid = \"depth_sensor\"\n",
    "depth_sensor_spec.sensor_type = habitat_sim.SensorType.DEPTH\n",
    "depth_sensor_spec.resolution = [sensor_settings[\"height\"], sensor_settings[\"width\"]]\n",
    "depth_sensor_spec.position = [0.0, sensor_settings[\"sensor_height\"], 0.0]\n",
    "depth_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "\n",
    "sensor_specs = [rgb_sensor_spec, depth_sensor_spec]\n",
    "                #]\n",
    "                #, depth_sensor_spec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24899868-2f3b-4030-b69a-11713902b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_settings = {\n",
    "    \"action_space\": {\n",
    "        \"move_forward\": 0.25, \"move_backward\": 0.25,  # Distance to cover in a move action in meters\n",
    "        \"turn_left\": 30.0, \"turn_right\": 30,  # Angles to cover in a turn action in degrees\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create an agent configuration\n",
    "agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "agent_cfg.action_space = {\n",
    "    k: habitat_sim.agent.ActionSpec(\n",
    "        k, habitat_sim.agent.ActuationSpec(amount=v)\n",
    "    ) for k, v in agent_settings[\"action_space\"].items()\n",
    "}\n",
    "agent_cfg.sensor_specifications = sensor_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a1e8e3-cd4a-49f1-96de-f40dda6c2992",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Simulator configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4366e1-aadd-47e9-be1e-494052353d01",
   "metadata": {},
   "source": [
    "This section defines which 3D scene to load in the simulator.  \r\n",
    "To switch scenes, **comment or uncomment** the desired `scene_id` path in `sim_settings`.  \r\n",
    "You can also add custom scene paths here and select which one is used during simulation by setting it in `\"scene_id\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7675b883-b507-4d6c-93ea-f353948c287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_settings = {\n",
    "    \"default_agent\": 0,  # Index of the default agent\n",
    "    \"scene_id\": \"data/scene_datasets/gibson/Edgemere.glb\",\n",
    "                # \"data/scene_datasets/gibson/Cantwell.glb\"\n",
    "    \"enable_physics\": False,  # kinematics only\n",
    "    \"seed\": 42  # used in the random navigation\n",
    "}\n",
    "\n",
    "# Create a simulator backend configuration\n",
    "sim_cfg = habitat_sim.SimulatorConfiguration()\n",
    "sim_cfg.scene_id = sim_settings[\"scene_id\"]\n",
    "sim_cfg.enable_physics = sim_settings[\"enable_physics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c26fbca-671d-45b1-bcca-c81b955ebb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configuration for the simulator\n",
    "cfg = habitat_sim.Configuration(sim_cfg, [agent_cfg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651d413-72bf-4cdb-88bd-df2980f0c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sim.close()\n",
    "except NameError:\n",
    "    pass\n",
    "sim = habitat_sim.Simulator(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826945af-9890-46fe-95b3-edd0c2bdee43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Wrapping simulator in gym.env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd013065-9b59-4c22-8d32-798b14e5d4e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Custom Gym Environments for Hide & Seek in Habitat-Sim\n",
    "\n",
    "This section defines multiple `gym.Env` environments for training agents with Stable-Baselines3 in Habitat-Sim.  \n",
    "Each environment varies based on:\n",
    "\n",
    "- **Task Type**: *Seek* (find the target) or *Hide* (avoid the target)\n",
    "- **Sensor Modalities**: RGB only or RGB + Depth\n",
    "\n",
    "##### Available Environments:\n",
    "- `RedBallEnv`: **RGB + Depth**, task = *Seek* the red ball\n",
    "- `RedBallEnvRGB`: **RGB only**, task = *Seek* the red ball\n",
    "- `HideFromBallEnv`: **RGB + Depth**, task = *Hide* from the red ball\n",
    "\n",
    "Use `DummyVecEnv` to wrap these environments for compatibility with SB3 algorithms.\n",
    "\n",
    "**Note**: You can use different 3D target objects by changing the file path in the environment’s `reset()` method,  \n",
    "in the `self.sim_rom.add_target(...)` call. Uncomment or modify the desired `.glb` file path depending on the object you want to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d145b269-1ad1-4fca-8de3-f79f5598a8d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Seeking environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6dab67-aa54-41fc-af37-0fd8acc40169",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### RGB + Depth sensors agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468fe55-4e9c-4fad-a53d-7d5b19283ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RgbDepthSeekingEnv(gym.Env):\n",
    "    def __init__(self, sim: habitat_sim.Simulator, sim_rom: RunTimeObjectManager):\n",
    "        super().__init__()\n",
    "        self.sim = sim\n",
    "        self.sim_rom = sim_rom\n",
    "        self.agent = sim.get_agent(0)\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 500\n",
    "\n",
    "        # RGB + Depth as 4 channels\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255, shape=(256, 256, 4), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "        # Discrete action space\n",
    "        self.actions = {\n",
    "            0: \"move_forward\",\n",
    "            1: \"turn_left\",\n",
    "            2: \"turn_right\",\n",
    "            3: \"move_backward\",\n",
    "            4: \"done\",\n",
    "        }\n",
    "        self.action_space = spaces.Discrete(len(self.actions))\n",
    "        self.spl = -1\n",
    "\n",
    "        agent_state = habitat_sim.AgentState()\n",
    "        agent_state.position = self.sim.pathfinder.get_random_navigable_point()\n",
    "        self.agent.set_state(agent_state)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        \n",
    "        self.sim.reset()\n",
    "        agent_state = habitat_sim.AgentState()\n",
    "        agent_state.position = self.sim.pathfinder.get_random_navigable_point()\n",
    "        self.agent.set_state(agent_state)\n",
    "        #self.agent.set_state(habitat_sim.AgentState())\n",
    "        self.step_count = 0\n",
    "\n",
    "        # Delete and add red target\n",
    "        self.sim_rom.delete_added_target()\n",
    "        \n",
    "        # The object will be placed at a random navigable position defined by 'target_position'.\n",
    "        target_position = self.sim.pathfinder.get_random_navigable_point()\n",
    "\n",
    "        # Add a target object to the scene.\n",
    "        # You can switch between different target models by commenting/uncommenting the desired .glb file path.\n",
    "        self.sim_rom.add_target(\n",
    "            '/workspace/habitat-sim/data/test_assets/objects/TurtleBot4_Lite_model_with_red_ball.glb',\n",
    "            #'/workspace/habitat-sim/data/test_assets/objects/TurtleBot4_Lite_model_new.glb',\n",
    "            #'/workspace/habitat-sim/data/test_assets/objects/red_ball.glb',\n",
    "            target_position\n",
    "        )\n",
    "        self.target_position = target_position  # Keep track for reward\n",
    "        print(\"[INFO] Currently in reset(), target_position={}\".format(target_position))\n",
    "\n",
    "        self.start_position = self.agent.get_state().position\n",
    "        shortest_path_request = habitat_sim.ShortestPath()\n",
    "        shortest_path_request.requested_start = self.start_position\n",
    "        shortest_path_request.requested_end = self.target_position\n",
    "        \n",
    "        found_path = self.sim.pathfinder.find_path(shortest_path_request)\n",
    "        \n",
    "        self.geodesic_distance = shortest_path_request.geodesic_distance\n",
    "\n",
    "\n",
    "\n",
    "        self.success = False\n",
    "        self.episode_path_length = 0.0\n",
    "        self.last_position = self.start_position\n",
    "\n",
    "        print(f\"[INFO]: spl={self.spl}\") \n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        act_name = self.actions[action]\n",
    "\n",
    "        agent_pos = self.agent.get_state().position\n",
    "        dist_to_target = np.linalg.norm(agent_pos - np.array(self.target_position))\n",
    "        \n",
    "        done = False\n",
    "\n",
    "        if self.max_steps < self.step_count:\n",
    "            done = True\n",
    "            print(\"NO SUCCESS\")\n",
    "        else :     \n",
    "            \n",
    "            if (act_name == \"done\"):\n",
    "                done = (dist_to_target < 0.5)\n",
    "                if done: \n",
    "                    self.success = True\n",
    "                    print(\"SUCCESS !!!\")\n",
    "            else:\n",
    "                self.agent.act(act_name)\n",
    "\n",
    "        # Track distance moved\n",
    "        current_position = self.agent.get_state().position\n",
    "        step_distance = np.linalg.norm(current_position - self.last_position)\n",
    "        self.episode_path_length += step_distance\n",
    "        self.last_position = current_position\n",
    "        \n",
    "        obs = self._get_obs()\n",
    "        reward = -0.01 + self.success * 20\n",
    "        \n",
    "        self.step_count += 1\n",
    "        \n",
    "        info = {\n",
    "            \"success\": self.success,\n",
    "            \"geodesic_distance\": self.geodesic_distance,\n",
    "            \"episode_path_length\": self.episode_path_length,\n",
    "            \"spl\": self.success * (self.geodesic_distance / max(self.episode_path_length, self.geodesic_distance + 1e-6)),\n",
    "        }\n",
    "        \n",
    "        self.spl = info[\"spl\"]\n",
    "            \n",
    "        return obs, reward, done, False, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        observations = self.sim.get_sensor_observations()\n",
    "        rgb = observations[\"color_sensor\"]\n",
    "        depth = observations[\"depth_sensor\"]\n",
    "\n",
    "        rgb = rgb[..., :3].astype(np.uint8)\n",
    "        depth = np.clip(depth, 0, 10)\n",
    "        depth = (depth / 10.0 * 255).astype(np.uint8)\n",
    "        depth = np.expand_dims(depth, axis=2)\n",
    "\n",
    "        return np.concatenate([rgb, depth], axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1c0ab-7198-4114-994f-21b9c5519375",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### RGB sensor agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb8d4b-fae2-4abc-9f11-3279ab51da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RgbSeekingEnv(gym.Env):\n",
    "    def __init__(self, sim: habitat_sim.Simulator, sim_rom: RunTimeObjectManager):\n",
    "        super().__init__()\n",
    "        self.sim = sim\n",
    "        self.sim_rom = sim_rom\n",
    "        self.agent = sim.get_agent(0)\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 500\n",
    "\n",
    "        # RGB only observation space with 3 channels (Height x Width x Channels)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255, shape=(256, 256, 3), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "        # Define discrete action space with 5 actions:\n",
    "        # move_forward, turn_left, turn_right, move_backward, and done (end episode)\n",
    "        self.actions = {\n",
    "            0: \"move_forward\",\n",
    "            1: \"turn_left\",\n",
    "            2: \"turn_right\",\n",
    "            3: \"move_backward\",\n",
    "            4: \"done\",\n",
    "        }\n",
    "        self.action_space = spaces.Discrete(len(self.actions))\n",
    "        self.spl = -1  # Initialize Success weighted by Path Length metric\n",
    "\n",
    "        # Initialize agent's position randomly on a navigable point in the environment\n",
    "        agent_state = habitat_sim.AgentState()\n",
    "        agent_state.position = self.sim.pathfinder.get_random_navigable_point()\n",
    "        self.agent.set_state(agent_state)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        # Reset the simulator environment and agent state\n",
    "        self.sim.reset()\n",
    "\n",
    "        # Randomly place the agent at a new navigable point\n",
    "        agent_state = habitat_sim.AgentState()\n",
    "        agent_state.position = self.sim.pathfinder.get_random_navigable_point()\n",
    "        self.agent.set_state(agent_state)\n",
    "        self.step_count = 0\n",
    "\n",
    "        # Remove any previously added target objects from the scene\n",
    "        self.sim_rom.delete_added_target()\n",
    "\n",
    "        # Select a new random target position on navigable terrain\n",
    "        target_position = self.sim.pathfinder.get_random_navigable_point()\n",
    "\n",
    "        # Add a target object to the scene.\n",
    "        # You can switch between different target models by commenting/uncommenting the desired .glb file path.        \n",
    "        self.sim_rom.add_target(\n",
    "            #'/workspace/habitat-sim/data/test_assets/objects/TurtleBot4_Lite_model_with_red_ball.glb',\n",
    "            #'/workspace/habitat-sim/data/test_assets/objects/TurtleBot4_Lite_model_new.glb',\n",
    "            '/workspace/habitat-sim/data/test_assets/objects/red_ball.glb',\n",
    "            target_position\n",
    "        )\n",
    "        self.target_position = target_position  # Store target position for distance calculations\n",
    "\n",
    "        # Record the agent's starting position for navigation metrics\n",
    "        self.start_position = self.agent.get_state().position\n",
    "\n",
    "        # Setup shortest path query from start to target to measure geodesic distance\n",
    "        shortest_path_request = habitat_sim.ShortestPath()\n",
    "        shortest_path_request.requested_start = self.start_position\n",
    "        shortest_path_request.requested_end = self.target_position\n",
    "        self.sim.pathfinder.find_path(shortest_path_request)\n",
    "        self.geodesic_distance = shortest_path_request.geodesic_distance\n",
    "\n",
    "        # Initialize episode success flag and path length tracking\n",
    "        self.success = False\n",
    "        self.episode_path_length = 0.0\n",
    "        self.last_position = self.start_position\n",
    "\n",
    "        # Return the initial RGB observation and empty info dict\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map discrete action index to action name\n",
    "        act_name = self.actions[action]\n",
    "\n",
    "        # Calculate Euclidean distance between agent and target positions\n",
    "        agent_pos = self.agent.get_state().position\n",
    "        dist_to_target = np.linalg.norm(agent_pos - np.array(self.target_position))\n",
    "\n",
    "        done = False\n",
    "\n",
    "        # Terminate episode if max steps exceeded\n",
    "        if self.step_count >= self.max_steps:\n",
    "            print(\"NO SUCCESS\")\n",
    "            done = True\n",
    "        elif act_name == \"done\":\n",
    "            # Check if agent claims to be done and is close enough to the target\n",
    "            done = (dist_to_target < 0.5)\n",
    "            if done:\n",
    "                self.success = True\n",
    "                print(\"SUCCESS !!!\")\n",
    "        else:\n",
    "            # Perform the agent action in the environment\n",
    "            self.agent.act(act_name)\n",
    "\n",
    "        # Track incremental distance traveled by the agent this step\n",
    "        current_position = self.agent.get_state().position\n",
    "        step_distance = np.linalg.norm(current_position - self.last_position)\n",
    "        self.episode_path_length += step_distance\n",
    "        self.last_position = current_position\n",
    "\n",
    "        # Get new observation from simulator after action\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Simple reward: small negative reward each step + big reward on success\n",
    "        reward = -0.01 + self.success * 20\n",
    "\n",
    "        self.step_count += 1\n",
    "\n",
    "        # Info dictionary with success flag, path length, geodesic distance, and SPL metric\n",
    "        info = {\n",
    "            \"success\": self.success,\n",
    "            \"geodesic_distance\": self.geodesic_distance,\n",
    "            \"episode_path_length\": self.episode_path_length,\n",
    "            \"spl\": self.success * (self.geodesic_distance / max(self.episode_path_length, self.geodesic_distance + 1e-6)),\n",
    "        }\n",
    "        self.spl = info[\"spl\"]\n",
    "\n",
    "        # Return observation, reward, done flag, truncated flag (False), and info dict\n",
    "        return obs, reward, done, False, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Fetch sensor observations from the simulator\n",
    "        observations = self.sim.get_sensor_observations()\n",
    "\n",
    "        # Extract RGB image and convert to uint8 format (3 channels)\n",
    "        rgb = observations[\"color_sensor\"]\n",
    "        rgb = rgb[..., :3].astype(np.uint8)\n",
    "\n",
    "        return rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9a6b5-ccf8-4afa-9d6e-e3c869ada160",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hiding environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c36c6-b50a-4a6d-b0b3-cf0936197fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HidingEnv(gym.Env):\n",
    "    def __init__(self, sim: habitat_sim.Simulator, sim_rom: RunTimeObjectManager):\n",
    "        super().__init__()\n",
    "        self.sim = sim\n",
    "        self.sim_rom = sim_rom\n",
    "        self.agent = sim.get_agent(0)\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 500\n",
    "\n",
    "        # Observation space includes RGB + Depth channels (4 channels total)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255, shape=(256, 256, 4), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "        # Define discrete action space: move_forward, turn_left, turn_right, move_backward\n",
    "        self.actions = {\n",
    "            0: \"move_forward\",\n",
    "            1: \"turn_left\",\n",
    "            2: \"turn_right\",\n",
    "            3: \"move_backward\",\n",
    "        }\n",
    "        self.action_space = spaces.Discrete(len(self.actions))\n",
    "\n",
    "        # Parameters to control seeker object's star-shaped movement pattern\n",
    "        self.step_in_leg = 0    # Current step count within one leg of star movement\n",
    "        self.leg_length = 4     # Number of steps per leg segment\n",
    "        self.legs_done = 0      # Number of completed legs in star pattern\n",
    "        self.total_legs = 5     # Total legs in star movement (360° / 72° per leg)\n",
    "        self.done = False       # Flag indicating episode completion\n",
    "        self.success = True     # Flag for success status (starts True but may become False if seeker finds agent)\n",
    "\n",
    "        # The seeker object will be added as a target during reset()\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        # Reset environment to initial state for new episode\n",
    "        self.sim.reset()\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "        self.success = True\n",
    "        self.step_in_leg = 0\n",
    "        self.legs_done = 0\n",
    "\n",
    "        # Randomly position agent (hider) on navigable point in the environment\n",
    "        agent_state = habitat_sim.AgentState()\n",
    "        agent_state.position = self.sim.pathfinder.get_random_navigable_point()\n",
    "        self.agent.set_state(agent_state)\n",
    "\n",
    "        # Remove any previously added seeker target from the scene\n",
    "        self.sim_rom.delete_added_target()\n",
    "\n",
    "        # Add seeker object at fixed position in the environment\n",
    "        seeker_pos = [-1.2753457 ,  0.18560381,  0.6338574 ] # Fixed seeker start position near the center of Edgemere environment\n",
    "        # You can switch between different seeker models by commenting/uncommenting the desired .glb file path. \n",
    "        self.sim_rom.add_target(\n",
    "            '/workspace/habitat-sim/data/test_assets/objects/TurtleBot4_Lite_model_with_red_ball.glb',\n",
    "            #'/workspace/habitat-sim/data/test_assets/objects/TurtleBot4_Lite_model_new.glb',\n",
    "            #'/workspace/habitat-sim/data/test_assets/objects/red_ball.glb',\n",
    "            seeker_pos\n",
    "        )\n",
    "\n",
    "        # Store reference to seeker object and set its motion type to kinematic (controlled externally)\n",
    "        self.seeker_obj = self.sim_rom._added_target\n",
    "        self.seeker_obj.motion_type = habitat_sim.physics.MotionType.KINEMATIC\n",
    "\n",
    "        # Return initial RGB + Depth observation and empty info dictionary\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Raise error if called after episode is done without reset\n",
    "        if self.done:\n",
    "            raise RuntimeError(\"Episode done, reset required\")\n",
    "\n",
    "        # Execute agent (hider) action in the environment\n",
    "        act_name = self.actions[action]\n",
    "        self.agent.act(act_name)\n",
    "\n",
    "        # Move seeker object in a star pattern if not done\n",
    "        if self.legs_done < self.total_legs:\n",
    "            if self.step_in_leg < self.leg_length:\n",
    "                # Calculate forward direction vector relative to seeker orientation\n",
    "                forward_vec = self.seeker_obj.rotation.transform_vector(Vector3(0, 0, -1))\n",
    "                new_pos = self.seeker_obj.translation + forward_vec * 0.5  # Move forward by 0.5 units\n",
    "                \n",
    "                # Check if the new position is navigable by seeker\n",
    "                if self.sim.pathfinder.is_navigable(new_pos):\n",
    "                    self.seeker_obj.translation = new_pos  # Update seeker position\n",
    "                    self.step_in_leg += 1                  # Increment step within current leg\n",
    "                else:\n",
    "                    # If blocked, end current leg early and prepare to turn\n",
    "                    self.step_in_leg = self.leg_length\n",
    "            else:\n",
    "                # Turn seeker by 72 degrees to start new leg of star pattern\n",
    "                angle_rad = Rad(np.deg2rad(72))\n",
    "                axis = Vector3(0, 1, 0)  # Y-axis for vertical rotation\n",
    "                turn_quat = Quaternion.rotation(angle_rad, axis)\n",
    "                self.seeker_obj.rotation = turn_quat * self.seeker_obj.rotation  # Apply rotation\n",
    "\n",
    "                # Reset step counter and increment number of legs completed\n",
    "                self.step_in_leg = 0\n",
    "                self.legs_done += 1\n",
    "        else:\n",
    "            # Star movement finished; mark episode done\n",
    "            self.done = True\n",
    "\n",
    "        # Calculate Euclidean distance between seeker and agent positions\n",
    "        seeker_pos = np.array(self.seeker_obj.translation)\n",
    "        agent_pos = np.array(self.agent.get_state().position)\n",
    "        dist = np.linalg.norm(seeker_pos - agent_pos)\n",
    "\n",
    "        # If seeker is within 0.5 units of agent, seeker \"found\" hider: big penalty and end episode\n",
    "        if dist < 0.5:\n",
    "            print(\"[INFO] we are close\")\n",
    "            reward = -20\n",
    "            self.done = True\n",
    "            self.success = False\n",
    "        else:\n",
    "            # Small positive reward for each step survived without being found\n",
    "            reward = 0.01\n",
    "\n",
    "        # Increment step count and check max step termination\n",
    "        self.step_count += 1\n",
    "        if self.step_count > self.max_steps:\n",
    "            self.done = True\n",
    "\n",
    "        # Get latest observation from simulator\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Info dictionary with success flag and current seeker-agent distance\n",
    "        info = {\"success\": self.success, \"distance\": dist}\n",
    "\n",
    "        # Return observation, reward, done flag, truncated flag (False), and info dictionary\n",
    "        return obs, reward, self.done, False, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Retrieve raw sensor observations from the simulator\n",
    "        observations = self.sim.get_sensor_observations()\n",
    "\n",
    "        # Extract RGB image and convert to uint8 (3 channels)\n",
    "        rgb = observations[\"color_sensor\"][..., :3].astype(np.uint8)\n",
    "\n",
    "        # Extract depth sensor data, clip to max 10 meters, scale to 0-255 range, and expand dims for concatenation\n",
    "        depth = observations[\"depth_sensor\"]\n",
    "        depth = np.clip(depth, 0, 10)\n",
    "        depth = (depth / 10.0 * 255).astype(np.uint8)\n",
    "        depth = np.expand_dims(depth, axis=2)\n",
    "\n",
    "        # Concatenate RGB and depth channels to produce final 4-channel observation\n",
    "        return np.concatenate([rgb, depth], axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfe5e84-8027-4eb4-a54a-8cb83fd5de8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Simulation Visualization and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709de65-f8e5-4c22-8b95-7a0e751cbb18",
   "metadata": {},
   "source": [
    "This section provides tools to visualize RGB and Depth observations from the agent's sensors.  \r\n",
    "- `display_obs` shows RGB and Depth images side-by-side.  \r\n",
    "- You can manually move the agent and inspect its state.  \r\n",
    "- Targets and objects can be added or moved using `sim_rom`.  \r\n",
    "- Use this for quick debugging and verifying sensor data before training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55800267-2850-465b-bf1e-34b3216b43ba",
   "metadata": {},
   "source": [
    "> **Note:** This section is for debugging and testing the environment only.  \r\n",
    "> It should not be run before or during training if you only want to train the model\r\n",
    "*\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b666e6a-eeca-47f3-8123-ef15d40acf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function for displaying observations\n",
    "def display_obs(rgb_obs: np.ndarray, depth_obs: np.ndarray):\n",
    "    img_arr, title_arr = [], []\n",
    "    \n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGBA\")\n",
    "    img_arr.append(rgb_img)\n",
    "    title_arr.append(\"rgb\")\n",
    "    \n",
    "    depth_img = Image.fromarray((depth_obs / 10 * 255).astype(np.uint8), mode=\"L\")\n",
    "    img_arr.append(depth_img)\n",
    "    title_arr.append(\"depth\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, (img, title) in enumerate(zip(img_arr, title_arr)):\n",
    "        ax = plt.subplot(1, 2, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(title)\n",
    "        plt.imshow(img)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ac81e-0827-4b28-84be-2b9f8609853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = sim.initialize_agent(sim_settings[\"default_agent\"])  # Get our default agent\n",
    "agent_state = habitat_sim.AgentState()\n",
    "agent_state.position = np.array([-4.69643, 0.15825, -2.90618])  # Position in world coordinate\n",
    "agent.set_state(agent_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac67ccf-c75d-4523-8ce1-50701dacb315",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_rom = RunTimeObjectManager(sim)\n",
    "env = RedBallEnv(sim, sim_rom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b00b2c-5e32-4033-aef3-92f0d6abbd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_position = [-2.3028612 ,  0.18560381, -0.11677533]\n",
    "#target_position[1] = 0\n",
    "#sim_rom.add_target('/workspace/habitat-sim/data/test_assets/objects/TurtleBot4_Lite_model_with_red_ball.glb',target_position)\n",
    "#sim_rom.add_target('/workspace/habitat-sim/data/test_assets/objects/TurtleBot4_Lite_model_new.glb', target_position)\n",
    "# sim_rom.add_target('/workspace/habitat-sim/data/test_assets/objects/TurtleBot4_Lite_model.glb', [-2.3028612 ,  0  , -4.09697294])\n",
    "\n",
    "# sim.agents[0].act(\"turn_left\")\n",
    "#sim.agents[0].act(\"turn_left\")\n",
    "# sim.agents[0].act(\"turn_right\")\n",
    "# sim.agents[0].act(\"turn_right\")\n",
    "# sim.agents[0].act(\"move_forward\")\n",
    "#sim.agents[0].act(\"move_forward\")\n",
    "# sim.agents[0].act(\"move_backward\")\n",
    "# sim.agents[0].act(\"move_backward\")\n",
    "\n",
    "observations = sim.get_sensor_observations()\n",
    "rgb_obs = observations[\"color_sensor\"]\n",
    "depth_obs = observations[\"depth_sensor\"]\n",
    "\n",
    "display_obs(rgb_obs, depth_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed5c51-8593-498f-b065-82e8bb066f13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad979cc-73bd-4281-9fab-83cd6618ef3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PPO Training Runs Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58052cf8-5bd1-45e0-ae28-dcae68a8aa6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Seeking Task\n",
    "\n",
    "- **ppo_random_ball_pos_v0**  \n",
    "  Red ball target — RGB + Depth sensors  \n",
    "  Ball random, agent fixed — Edgemere — `CnnPolicy`, batch size 128  \n",
    "  **SPL:** 0.837 &nbsp;&nbsp; **SR:** 0.992\n",
    "\n",
    "- **ppo_random_ball_and_robot_pos_rgb_v01**  \n",
    "  Red ball target — RGB only  \n",
    "  Ball and agent random — Edgemere — `CnnPolicy`, batch size 128  \n",
    "  **SPL:** 0.663 &nbsp;&nbsp; **SR:** 0.992\n",
    "\n",
    "- **ppo_random_ball_and_robot_pos_v02**  \n",
    "  Red ball target — RGB + Depth sensors  \n",
    "  Ball and agent random — Edgemere — `CnnPolicy`, batch size 128  \n",
    "  **SPL:** 0.844 &nbsp;&nbsp; **SR:** 0.979\n",
    "\n",
    "- **ppo_robot_seek**  \n",
    "  Red ball target — RGB + Depth sensors  \n",
    "  Ball and agent random — Edgemere — `CnnPolicy`, batch size 128  \n",
    "  **SPL:** 0.168 &nbsp;&nbsp; **SR:** 0.379\n",
    "\n",
    "- **ppo_robot_and_red_ball_seek**  \n",
    "  Robot + red ball target — RGB + Depth sensors  \n",
    "  Ball and agent random — Edgemere — `CnnPolicy`, batch size 128  \n",
    "  **SPL:** 0.839 &nbsp;&nbsp; **SR:** 0.997\n",
    "\n",
    "#### Hiding Task\n",
    "\n",
    "- **ppo_robot_and_red_ball_hide**  \n",
    "  Seeker = Robot + red ball — RGB + Depth sensors  \n",
    "  Ball and agent random — Edgemere — `CnnPolicy`, batch size 128  \n",
    "  **SR:** 0.854\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38863aca-d144-4471-a626-7e523f45a976",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PPO Training Phase\r\n",
    "\n",
    "This section launches the training of a PPO agent using Stable-Baselines3.\n",
    "\n",
    "- A custom Gym environment (`RgbDepthSeekingEnv`, `RgbSeekingEnv`, or `HidingEnv`) is wrapped in `DummyVecEnv` for compatibility.\n",
    "- The PPO model uses a `CnnPolicy`, suitable for visual input (RGB and/or Depth).\n",
    "- The agent is trained for **1,000,000 timesteps** with a batch size of **128**.\n",
    "- After training, the model is saved to disk.\n",
    "\n",
    "To switch environments (seeking or hiding), comment/uncomment the corresponding line during instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a6bda-4e90-4ce9-b85d-7ab536eff095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always run this cell when you need to create or reset the environment (not just during training)\n",
    "sim_rom = RunTimeObjectManager(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402dfb15-17d5-464b-8e47-11c8f0682d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = RgbDepthSeekingEnv(sim, sim_rom)\n",
    "#RgbSeekingEnv(sim, sim_rom)\n",
    "#HidingEnv(sim, sim_rom)\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "model = PPO('CnnPolicy', vec_env, n_steps = 500, batch_size = 128, verbose=1)\n",
    "model.learn(total_timesteps=1_000_000)\n",
    "model.save(\"ppo_robot_and_red_ball_hide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040214e9-f3e4-4c29-81ed-46ac2f6d0bd0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Fine-Tuning Phase\r\n",
    "\r\n",
    "This section continues training (fine-tuning) a previously saved PPO model.\r\n",
    "\r\n",
    "- The model is loaded from a previous ruppo_robot_and_red_ball_hide_new`).\r\n",
    "- A new environmeHidingEnvllEnv`) is defined and wrapped in `DummyVecEnv`.\r\n",
    "- The model is linked to the new environment via `set_env()`.\r\n",
    "- Training resumes5for **800,000 timesteps**.\r\n",
    "- The updated model is saved under a newppo_robot_and_red_ball_hide_fine_tunedwell_v03`).\r\n",
    "\r\n",
    "Useful for adapting a pre-trained agent to a new environment or slightly different tsk setup.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be06a8f-f030-4058-af0d-d1b2a77a188f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo_robot_and_red_ball_hide\")\n",
    "env = HidingEnv(sim, sim_rom)\n",
    "new_vec_env = DummyVecEnv([lambda: env])\n",
    "model.set_env(new_vec_env)\n",
    "model.learn(total_timesteps=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64981542-42b2-4365-a419-31b0124ad6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_robot_and_red_ball_hide_fine_tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93bb1e6-ad1a-47ec-8d39-f40174def808",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Evaluation Phase (Prediction & Metrics)\n",
    "\n",
    "- Run the trained agent using `model.predict()`.\n",
    "- Collect metrics separately:\n",
    "  - **Success Rate (SR)**: For both Hide and Seek tasks.\n",
    "  - **SPL**: Only for Seek tasks.\n",
    "\n",
    "> Note: Comment or uncomment SPL-related lines depending on the task (SPL only applies to Seek)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1144d-a85b-4eba-893e-e71aa99e35e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = HideSeekEnv(sim, sim_rom)\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = PPO.load(\"ppo_robot_and_red_ball_hide\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "nb_episode = 1000\n",
    "\n",
    "# Use `spls` for Seeking tasks only (SPL is not relevant for Hiding)\n",
    "# spls = []\n",
    "srs = []\n",
    "\n",
    "# Evaluate model across multiple episodes\n",
    "for episode in range(nb_episode):\n",
    "    print(f\"[TESTING INFO] episode={episode}/{nb_episode}\")\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, done, info = vec_env.step(action)\n",
    "    # spls.append(info[0][\"spl\"]) # Uncomment for Seeking tasks\n",
    "    srs.append(info[0][\"success\"])\n",
    "\n",
    "# Optional: visualize predictions frame by frame\n",
    "for i in range(1000):\n",
    "    print(f\"[INFO] iteration={i}\")\n",
    "    time.sleep(.1)\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    print(action)\n",
    "    plt.imshow(obs.squeeze()[:,:,:3])\n",
    "    plt.show()\n",
    "\n",
    "# Print metrics (SPL for Seeking, SR for both)\n",
    "# print(f\"SPL Mean : {np.mean(spls)}\")\n",
    "print(f\"SR Mean : {np.mean(srs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52363ed-41ee-4008-9013-dd74036b3118",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Video Generation over 10 Episodes\r\n",
    "\r\n",
    "- Run the agent for 10 episodes, collecting RGB and Depth observations at each step.\r\n",
    "- Store observations as image frames for video creation.\r\n",
    "- Save two videos:\r\n",
    "  - **RGB video**: Converts RGB frames to BGR for OpenCV.\r\n",
    "  - **Depth video**: Converts single-channel depth frames to 3-channel grayscale for saving.\r\n",
    "\r\n",
    "> Update file paths and parameters (fps, resolution) asneeded.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca838f0-5550-4b7e-9b48-22a3a4444782",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "images_rgb = []\n",
    "images_depth = []\n",
    "\n",
    "model = PPO.load(\"ppo_random_ball_pos_v02\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "nb_episode = 10\n",
    "\n",
    "for _ in range(nb_episode):\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, done, info = vec_env.step(action)\n",
    "        rgb = obs[0, ..., :3]     \n",
    "        depth = obs[0, ..., 3]    \n",
    "        images_rgb.append(rgb)\n",
    "        images_depth.append(depth)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9cc238-3584-429c-98fd-50f97d626c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video parameters\n",
    "fps = 10\n",
    "height, width, _ = images_rgb[0].shape\n",
    "\n",
    "# === RGB Video ===\n",
    "rgb_writer = cv2.VideoWriter(\"videos/rgb_video_10_ep_v0.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "for rgb in images_rgb:\n",
    "    # OpenCV expects images in BGR format\n",
    "    bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
    "    rgb_writer.write(bgr)\n",
    "\n",
    "rgb_writer.release()\n",
    "\n",
    "# === Depth Video ===\n",
    "depth_writer = cv2.VideoWriter(\"videos/depth_video_10_ep_v0.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "for depth in images_depth:\n",
    "    # If depth is 2D (H, W), convert to 3-channel grayscale for saving\n",
    "    depth_rgb = cv2.cvtColor(depth, cv2.COLOR_GRAY2BGR)\n",
    "    depth_writer.write(depth_rgb)\n",
    "\n",
    "depth_writer.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
