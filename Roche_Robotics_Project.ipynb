{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a658b9-1daa-4417-8eba-fe2081277340",
   "metadata": {},
   "source": [
    "# Roche robotics poject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3b4cc-d58d-4e3c-bf03-d8d054d77e87",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a3bad3e-cf4a-4e23-b219-9fd43199532a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither `ifconfig` (`ifconfig -a`) nor `ip` (`ip address show`) commands are available, listing network interfaces is likely to fail\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from habitat_baselines.config.default import get_config\n",
    "from habitat_baselines.rl.ppo.ppo_trainer import PPOTrainer\n",
    "from habitat_baselines.common.construct_vector_env import construct_envs\n",
    "from habitat_baselines.config.default import get_config\n",
    "\n",
    "from pg.base_pg import BasePolicyGradient\n",
    "from pg.base_pg_trainer import BasePolicyGradientTrainer\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a56b7-dd0f-4cc6-8f7e-cfcb26a1f120",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "065f79ce-1509-4716-8875-ad7cbaaafe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tensorboard(path, scalars, prefix=\"\"):\n",
    "    \"\"\"returns a dictionary of pandas dataframes for each requested scalar\"\"\"\n",
    "    ea = event_accumulator.EventAccumulator(\n",
    "        path,\n",
    "        size_guidance={event_accumulator.SCALARS: 0},\n",
    "    )\n",
    "    _ = ea.Reload()\n",
    "    df = pd.DataFrame(ea.Scalars(scalars[0]))\n",
    "    df[f'{scalars[0]}'] = df['value']\n",
    "    df.drop(columns=['value'], inplace=True)\n",
    "    for k in scalars:\n",
    "        df[k] = pd.DataFrame(ea.Scalars(k))['value']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c124f396-b892-47a7-9c7e-f3770d39ef20",
   "metadata": {},
   "source": [
    "## Configuration 1 : PointNav, Empty Room"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f6688-fa79-41ed-8f4f-2df12ca5e617",
   "metadata": {},
   "source": [
    "### PPO Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b498bcaf-a121-4670-adc8-c5c4933d3657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_PPO_config():    \n",
    "    config = get_config(\"targetnav/comm_proj_discrete.yaml\")\n",
    "    #pointnav/ppo_pointnav.yaml\n",
    "    #targetnav/comm_proj_discrete.yaml\n",
    "    OmegaConf.set_readonly(config, False)\n",
    "    config.habitat_baselines.tensorboard_dir = \"tb/PPO\"\n",
    "    config.habitat_baselines.checkpoint_folder = \"data/PPO4_checkpoints\"\n",
    "    config.habitat_baselines.num_updates = -1\n",
    "    config.habitat_baselines.num_environments = 2\n",
    "    config.habitat_baselines.verbose = True\n",
    "    config.habitat_baselines.num_checkpoints = -1\n",
    "    config.habitat_baselines.checkpoint_interval = 1000000\n",
    "    config.habitat_baselines.total_num_steps = 150 * 1000\n",
    "    config.habitat.dataset.data_path = \"data/datasets/pointnav/simple_room/v0/{split}/empty_room.json.gz\"\n",
    "    config.habitat_baselines.load_resume_state_config = False\n",
    "    #config.habitat.simulator\n",
    "    \n",
    "    #config.habitat.simulator.scene = \"data/scene_datasets/simple_room/empty_room.glb\"\n",
    "    #config.habitat.simulator.scene = \"data/scene_datasets/simple_room/empty_room.scene_instance.json\"\n",
    "\n",
    "    OmegaConf.set_readonly(config, True)\n",
    "\n",
    "    return config\n",
    "\n",
    "config = build_PPO_config()\n",
    "om_config = OmegaConf.to_yaml(config)\n",
    "#print(om_config)\n",
    "      \n",
    "# Set randomness\n",
    "random.seed(config.habitat.seed)\n",
    "np.random.seed(config.habitat.seed)\n",
    "torch.manual_seed(config.habitat.seed)\n",
    "if (\n",
    "    config.habitat_baselines.force_torch_single_threaded\n",
    "    and torch.cuda.is_available()\n",
    "):\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "os.environ[\"MAGNUM_LOG\"] = \"quiet\"\n",
    "os.environ[\"HABITAT_SIM_LOG\"] = \"quiet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a51e1421-874c-49e2-83b5-6763944a49af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<habitat_sim._ext.habitat_sim_bindings.SimulatorConfiguration object at 0x7f8f07a18ef0>\n",
      "<habitat_sim._ext.habitat_sim_bindings.CameraSensorSpec object at 0x7f8f027ddc20>\n",
      "AgentConfiguration(height=1.5, radius=0.1, sensor_specifications=[<habitat_sim._ext.habitat_sim_bindings.CameraSensorSpec object at 0x7f8f027ddc20>], action_space={'move_forward': ActionSpec(name='move_forward', actuation=ActuationSpec(amount=0.25, constraint=None)), 'turn_left': ActionSpec(name='turn_left', actuation=ActuationSpec(amount=10.0, constraint=None)), 'turn_right': ActionSpec(name='turn_right', actuation=ActuationSpec(amount=10.0, constraint=None))}, body_type='cylinder')\n"
     ]
    }
   ],
   "source": [
    "import habitat_sim\n",
    "\n",
    "backend_cfg = habitat_sim.SimulatorConfiguration()\n",
    "print(backend_cfg)\n",
    "\n",
    "sem_cfg = habitat_sim.CameraSensorSpec()\n",
    "sem_cfg.uuid = \"semantic\"\n",
    "sem_cfg.sensor_type = habitat_sim.SensorType.SEMANTIC\n",
    "print(sem_cfg)\n",
    "\n",
    "agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "agent_cfg.sensor_specifications = [sem_cfg]\n",
    "print(agent_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08b884-0399-481d-ac6b-93f183ae0c68",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6b8d8e5-be85-4c09-8859-4e1062d101d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 09:52:25,698 config: habitat_baselines:\n",
      "  trainer_name: ppo\n",
      "  torch_gpu_id: 0\n",
      "  video_render_views: []\n",
      "  tensorboard_dir: tb/PPO\n",
      "  writer_type: wb\n",
      "  video_dir: video_dir\n",
      "  video_fps: 10\n",
      "  test_episode_count: -1\n",
      "  eval_ckpt_path_dir: data/new_checkpoints\n",
      "  num_environments: 2\n",
      "  num_processes: -1\n",
      "  checkpoint_folder: data/PPO4_checkpoints\n",
      "  num_updates: -1\n",
      "  num_checkpoints: -1\n",
      "  checkpoint_interval: 1000000\n",
      "  total_num_steps: 150000.0\n",
      "  log_interval: 25\n",
      "  video_interval: 150\n",
      "  log_file: train.log\n",
      "  force_blind_policy: false\n",
      "  verbose: true\n",
      "  eval_keys_to_include_in_name: []\n",
      "  force_torch_single_threaded: true\n",
      "  wb:\n",
      "    project_name: ''\n",
      "    entity: ''\n",
      "    group: ''\n",
      "    run_name: ''\n",
      "    run_id: ''\n",
      "  load_resume_state_config: false\n",
      "  eval:\n",
      "    split: val\n",
      "    use_ckpt_config: true\n",
      "    should_load_ckpt: true\n",
      "    evals_per_ep: 1\n",
      "    video_option: []\n",
      "  profiling:\n",
      "    capture_start_step: -1\n",
      "    num_steps_to_capture: -1\n",
      "  launch_eval_afterwards: true\n",
      "  rl:\n",
      "    preemption:\n",
      "      append_slurm_job_id: false\n",
      "      save_resume_state_interval: 100\n",
      "      save_state_batch_only: false\n",
      "    policy:\n",
      "      name: PointNavResNetPolicy\n",
      "      action_distribution_type: categorical\n",
      "      action_dist:\n",
      "        use_log_std: true\n",
      "        use_softplus: false\n",
      "        std_init: ???\n",
      "        log_std_init: 0.0\n",
      "        use_std_param: false\n",
      "        clamp_std: true\n",
      "        min_std: 1.0e-06\n",
      "        max_std: 1\n",
      "        min_log_std: -5\n",
      "        max_log_std: 2\n",
      "        action_activation: tanh\n",
      "        scheduled_std: false\n",
      "      obs_transforms: {}\n",
      "      hierarchical_policy: ???\n",
      "    ppo:\n",
      "      clip_param: 0.2\n",
      "      ppo_epoch: 4\n",
      "      num_mini_batch: 2\n",
      "      value_loss_coef: 0.5\n",
      "      entropy_coef: 0.01\n",
      "      lr: 0.00025\n",
      "      eps: 1.0e-05\n",
      "      max_grad_norm: 0.5\n",
      "      num_steps: 128\n",
      "      use_gae: true\n",
      "      use_linear_lr_decay: true\n",
      "      use_linear_clip_decay: true\n",
      "      gamma: 0.99\n",
      "      tau: 0.95\n",
      "      reward_window_size: 50\n",
      "      use_normalized_advantage: false\n",
      "      hidden_size: 512\n",
      "      entropy_target_factor: 0.0\n",
      "      use_adaptive_entropy_pen: false\n",
      "      use_clipped_value_loss: true\n",
      "      use_double_buffered_sampler: false\n",
      "    ddppo:\n",
      "      sync_frac: 0.6\n",
      "      distrib_backend: GLOO\n",
      "      rnn_type: GRU\n",
      "      num_recurrent_layers: 1\n",
      "      backbone: resnet18\n",
      "      pretrained_weights: data/ddppo-models/gibson-2plus-resnet50.pth\n",
      "      pretrained: false\n",
      "      pretrained_encoder: false\n",
      "      train_encoder: true\n",
      "      reset_critic: true\n",
      "      force_distributed: false\n",
      "    ver:\n",
      "      variable_experience: true\n",
      "      num_inference_workers: 2\n",
      "      overlap_rollouts_and_learn: false\n",
      "    auxiliary_losses: {}\n",
      "habitat:\n",
      "  seed: 100\n",
      "  env_task: GymHabitatEnv\n",
      "  env_task_gym_dependencies: []\n",
      "  env_task_gym_id: ''\n",
      "  environment:\n",
      "    max_episode_steps: 256\n",
      "    max_episode_seconds: 10000000\n",
      "    iterator_options:\n",
      "      cycle: true\n",
      "      shuffle: true\n",
      "      group_by_scene: true\n",
      "      num_episode_sample: -1\n",
      "      max_scene_repeat_episodes: -1\n",
      "      max_scene_repeat_steps: 10000\n",
      "      step_repetition_range: 0.2\n",
      "  simulator:\n",
      "    type: Sim-v0\n",
      "    action_space_config: v0\n",
      "    action_space_config_arguments: {}\n",
      "    forward_step_size: 0.25\n",
      "    create_renderer: false\n",
      "    requires_textures: true\n",
      "    auto_sleep: false\n",
      "    step_physics: true\n",
      "    concur_render: false\n",
      "    needs_markers: true\n",
      "    update_robot: true\n",
      "    scene: data/scene_datasets/habitat-test-scenes/van-gogh-room.glb\n",
      "    scene_dataset: default\n",
      "    additional_object_paths: []\n",
      "    seed: ${habitat.seed}\n",
      "    turn_angle: 30\n",
      "    tilt_angle: 15\n",
      "    default_agent_id: 0\n",
      "    debug_render: false\n",
      "    debug_render_robot: false\n",
      "    kinematic_mode: false\n",
      "    debug_render_goal: true\n",
      "    robot_joint_start_noise: 0.0\n",
      "    ctrl_freq: 120.0\n",
      "    ac_freq_ratio: 4\n",
      "    load_objs: false\n",
      "    hold_thresh: 0.15\n",
      "    grasp_impulse: 10000.0\n",
      "    agents:\n",
      "      main_agent:\n",
      "        height: 0.2\n",
      "        radius: 0.17\n",
      "        sim_sensors:\n",
      "          rgb_sensor:\n",
      "            type: HabitatSimRGBSensor\n",
      "            height: 128\n",
      "            width: 128\n",
      "            position:\n",
      "            - 0.0\n",
      "            - 0.15\n",
      "            - 0.0\n",
      "            orientation:\n",
      "            - 0.0\n",
      "            - 0.0\n",
      "            - 0.0\n",
      "            hfov: 54\n",
      "            sensor_subtype: PINHOLE\n",
      "            noise_model: None\n",
      "            noise_model_kwargs: {}\n",
      "          depth_sensor:\n",
      "            type: HabitatSimDepthSensor\n",
      "            height: 128\n",
      "            width: 128\n",
      "            position:\n",
      "            - 0.0\n",
      "            - 0.15\n",
      "            - 0.0\n",
      "            orientation:\n",
      "            - 0.0\n",
      "            - 0.0\n",
      "            - 0.0\n",
      "            hfov: 54\n",
      "            sensor_subtype: PINHOLE\n",
      "            noise_model: None\n",
      "            noise_model_kwargs: {}\n",
      "            min_depth: 0.0\n",
      "            max_depth: 10.0\n",
      "            normalize_depth: true\n",
      "        is_set_start_state: false\n",
      "        start_position:\n",
      "        - 0.0\n",
      "        - 0.0\n",
      "        - 0.0\n",
      "        start_rotation:\n",
      "        - 0.0\n",
      "        - 0.0\n",
      "        - 0.0\n",
      "        - 1.0\n",
      "        joint_start_noise: 0.1\n",
      "        robot_urdf: data/robots/hab_fetch/robots/hab_fetch.urdf\n",
      "        robot_type: FetchRobot\n",
      "        ik_arm_urdf: data/robots/hab_fetch/robots/fetch_onlyarm.urdf\n",
      "    agents_order:\n",
      "    - main_agent\n",
      "    habitat_sim_v0:\n",
      "      gpu_device_id: 0\n",
      "      gpu_gpu: false\n",
      "      allow_sliding: true\n",
      "      frustum_culling: true\n",
      "      enable_physics: false\n",
      "      physics_config_file: ./data/default.physics_config.json\n",
      "      leave_context_with_background_renderer: false\n",
      "      enable_gfx_replay_save: false\n",
      "    ep_info: null\n",
      "  task:\n",
      "    reward_measure: distance_to_goal_reward\n",
      "    success_measure: euclidean_success\n",
      "    success_reward: 2.5\n",
      "    slack_reward: -0.003\n",
      "    end_on_success: true\n",
      "    type: TargetNav-v0\n",
      "    lab_sensors: {}\n",
      "    measurements:\n",
      "      distance_to_goal:\n",
      "        type: DistanceToGoal\n",
      "        distance_to: POINT\n",
      "      euclidean_distance_to_goal:\n",
      "        type: EuclideanDistanceToGoal\n",
      "      euclidean_success:\n",
      "        type: EuclideanSuccess\n",
      "        success_distance: 0.4\n",
      "      nonstop_euclidean_success:\n",
      "        type: NonStopEuclideanSuccess\n",
      "        success_distance: 0.4\n",
      "      euclidean_spl:\n",
      "        type: EuclideanSPL\n",
      "      nonstop_euclidean_spl:\n",
      "        type: NonStopEuclideanSPL\n",
      "      distance_to_goal_reward:\n",
      "        type: DistanceToGoalReward\n",
      "      top_down_map:\n",
      "        type: TopDownMap\n",
      "        max_episode_steps: 256\n",
      "        map_padding: 3\n",
      "        map_resolution: 1024\n",
      "        draw_source: true\n",
      "        draw_border: true\n",
      "        draw_shortest_path: true\n",
      "        draw_view_points: true\n",
      "        draw_goal_positions: true\n",
      "        draw_goal_aabbs: true\n",
      "        fog_of_war:\n",
      "          draw: true\n",
      "          visibility_dist: 5.0\n",
      "          fov: 90\n",
      "    goal_sensor_uuid: pointgoal\n",
      "    count_obj_collisions: true\n",
      "    settle_steps: 5\n",
      "    constraint_violation_ends_episode: true\n",
      "    constraint_violation_drops_object: false\n",
      "    force_regenerate: false\n",
      "    should_save_to_cache: false\n",
      "    must_look_at_targ: true\n",
      "    object_in_hand_sample_prob: 0.167\n",
      "    min_start_distance: 3.0\n",
      "    render_target: true\n",
      "    physics_stability_steps: 1\n",
      "    num_spawn_attempts: 200\n",
      "    spawn_max_dists_to_obj: 2.0\n",
      "    base_angle_noise: 0.523599\n",
      "    ee_sample_factor: 0.2\n",
      "    ee_exclude_region: 0.0\n",
      "    base_noise: 0.05\n",
      "    spawn_region_scale: 0.2\n",
      "    joint_max_impulse: -1.0\n",
      "    desired_resting_position:\n",
      "    - 0.5\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    use_marker_t: true\n",
      "    cache_robot_init: false\n",
      "    success_state: 0.0\n",
      "    easy_init: false\n",
      "    should_enforce_target_within_reach: false\n",
      "    task_spec_base_path: habitat/task/rearrange/pddl/\n",
      "    task_spec: ''\n",
      "    pddl_domain_def: replica_cad\n",
      "    obj_succ_thresh: 0.3\n",
      "    enable_safe_drop: false\n",
      "    art_succ_thresh: 0.15\n",
      "    robot_at_thresh: 2.0\n",
      "    filter_nav_to_tasks: []\n",
      "    actions:\n",
      "      stop:\n",
      "        type: StopAction\n",
      "      move_forward:\n",
      "        type: MoveForwardAction\n",
      "      turn_left:\n",
      "        type: TurnLeftAction\n",
      "      turn_right:\n",
      "        type: TurnRightAction\n",
      "  dataset:\n",
      "    type: TargetNav-v0\n",
      "    split: train\n",
      "    scenes_dir: data/scene_datasets\n",
      "    content_scenes:\n",
      "    - '*'\n",
      "    data_path: data/datasets/pointnav/simple_room/v0/{split}/empty_room.json.gz\n",
      "  gym:\n",
      "    auto_name: ''\n",
      "    obs_keys: null\n",
      "    action_keys: null\n",
      "    achieved_goal_keys: []\n",
      "    desired_goal_keys: []\n",
      "\n",
      "2025-05-16 09:52:25,700 Removed metric top_down_map from metrics since it cannot be used during training.\n",
      "2025-05-16 09:52:25,700 Initializing dataset TargetNav-v0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument 'target_id' must be set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Build the trainer and start training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m PPOTrainer(config)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#trainer.agent.ini\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/miniconda/envs/habitat/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/habitat-lab/habitat-baselines/habitat_baselines/rl/ppo/ppo_trainer.py:792\u001b[0m, in \u001b[0;36mPPOTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Main method for training DD/PPO.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \n\u001b[1;32m    787\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    791\u001b[0m resume_state \u001b[38;5;241m=\u001b[39m load_resume_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m--> 792\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m count_checkpoints \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    795\u001b[0m prev_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/workspace/habitat-lab/habitat-baselines/habitat_baselines/rl/ppo/ppo_trainer.py:280\u001b[0m, in \u001b[0;36mPPOTrainer._init_train\u001b[0;34m(self, resume_state)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhabitat_baselines\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m    276\u001b[0m             logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    277\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemoved metric \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnon_scalar_metric_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from metrics since it cannot be used during training.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m             )\n\u001b[0;32m--> 280\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_envs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m action_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs\u001b[38;5;241m.\u001b[39maction_spaces[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_action_space \u001b[38;5;241m=\u001b[39m action_space\n",
      "File \u001b[0;32m/workspace/habitat-lab/habitat-baselines/habitat_baselines/rl/ppo/ppo_trainer.py:206\u001b[0m, in \u001b[0;36mPPOTrainer._init_envs\u001b[0;34m(self, config, is_eval)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_envs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers_ignore_signals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_slurm_batch_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43menforce_scenes_greater_eq_environments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/habitat-lab/habitat-baselines/habitat_baselines/common/construct_vector_env.py:40\u001b[0m, in \u001b[0;36mconstruct_envs\u001b[0;34m(config, workers_ignore_signals, enforce_scenes_greater_eq_environments)\u001b[0m\n\u001b[1;32m     38\u001b[0m scenes \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhabitat\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mcontent_scenes\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mhabitat\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mcontent_scenes:\n\u001b[0;32m---> 40\u001b[0m     scenes \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scenes_to_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhabitat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_environments \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_environments must be strictly positive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/habitat-lab/habitat-lab/habitat/datasets/pointnav/pointnav_dataset.py:58\u001b[0m, in \u001b[0;36mPointNavDatasetV1.get_scenes_to_load\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m read_write(cfg):\n\u001b[1;32m     57\u001b[0m     cfg\u001b[38;5;241m.\u001b[39mcontent_scenes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 58\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     has_individual_scene_files \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m     60\u001b[0m         dataset\u001b[38;5;241m.\u001b[39mcontent_scenes_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{scene}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     61\u001b[0m             data_path\u001b[38;5;241m=\u001b[39mdataset_dir\n\u001b[1;32m     62\u001b[0m         )\n\u001b[1;32m     63\u001b[0m     )\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_individual_scene_files:\n",
      "File \u001b[0;32m/workspace/habitat-lab/habitat-lab/habitat/datasets/pointnav/pointnav_dataset.py:101\u001b[0m, in \u001b[0;36mPointNavDatasetV1.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     99\u001b[0m datasetfile_path \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mdata_path\u001b[38;5;241m.\u001b[39mformat(split\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39msplit)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(datasetfile_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenes_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscenes_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Read separate file for each scene\u001b[39;00m\n\u001b[1;32m    104\u001b[0m dataset_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(datasetfile_path)\n",
      "File \u001b[0;32m/workspace/habitat-lab/habitat-lab/habitat/datasets/targetnav/targetnav_dataset.py:34\u001b[0m, in \u001b[0;36mTargetNavDatasetV0.from_json\u001b[0;34m(self, json_str, scenes_dir)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent_scenes_path \u001b[38;5;241m=\u001b[39m deserialized[CONTENT_SCENES_PATH_FIELD]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m deserialized[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisodes\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 34\u001b[0m     episode \u001b[38;5;241m=\u001b[39m \u001b[43mTargetNavigationEpisode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mepisode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scenes_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m episode\u001b[38;5;241m.\u001b[39mscene_id\u001b[38;5;241m.\u001b[39mstartswith(DEFAULT_SCENE_PATH_PREFIX):\n",
      "File \u001b[0;32m<attrs generated init habitat.tasks.nav.nav.TargetNavigationEpisode>:26\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, episode_id, scene_id, scene_dataset_config, additional_obj_config_paths, start_position, start_rotation, info, goals, start_room, shortest_paths, target_id, target_scale, scene_layout_id, scene_layout)\u001b[0m\n\u001b[1;32m     24\u001b[0m __attr_validator_start_rotation(\u001b[38;5;28mself\u001b[39m, __attr_start_rotation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_rotation)\n\u001b[1;32m     25\u001b[0m __attr_validator_goals(\u001b[38;5;28mself\u001b[39m, __attr_goals, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoals)\n\u001b[0;32m---> 26\u001b[0m \u001b[43m__attr_validator_target_id\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__attr_target_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m __attr_validator_target_scale(\u001b[38;5;28mself\u001b[39m, __attr_target_scale, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_scale)\n\u001b[1;32m     28\u001b[0m __attr_validator_scene_layout_id(\u001b[38;5;28mself\u001b[39m, __attr_scene_layout_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscene_layout_id)\n",
      "File \u001b[0;32m/workspace/habitat-lab/habitat-lab/habitat/core/utils.py:69\u001b[0m, in \u001b[0;36mnot_none_validator\u001b[0;34m(self, attribute, value)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnot_none_validator\u001b[39m(\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m: Any, attribute: attr\u001b[38;5;241m.\u001b[39mAttribute, value: Optional[Any]\n\u001b[1;32m     67\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattribute\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Argument 'target_id' must be set"
     ]
    }
   ],
   "source": [
    "# Build the trainer and start training\n",
    "trainer = PPOTrainer(config)\n",
    "trainer.train()\n",
    "\n",
    "#trainer.agent.ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d31b0e3a-be52-4509-aa59-8245bcfed0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainer.envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4835c589-7466-4a1d-bf3b-9000962d0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_rom = RunTimeObjectManager(sim)\n",
    "    sim_rom.delete_added_target()\n",
    "    sim_rom.add_target('/scratch/singh/morph_ws/habitat-lab/data/objects/targets/red_ball.glb', [0.0, 0.0, 0.0])\n",
    "    # '/workspace/habitat-sim/data/test_assets/objects/red_ball.glb'\n",
    "    sim.step_physics(physics_step_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d3893-6036-4d2e-b912-2f8fcc894904",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config_1_logs = parse_tensorboard(\"tb/PPO\", [\"metrics/spl\", \"reward\", \"learner/grad_norm\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0972ed1f-04c8-4c0b-92f7-7bf7d6a0e868",
   "metadata": {},
   "source": [
    "### Agent sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d9acc-263d-49ee-b4bb-a74afade703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = build_PPO_config()\n",
    "OmegaConf.set_readonly(config, False)\n",
    "\n",
    "# Creating the environment\n",
    "envs = construct_envs(config)\n",
    "\n",
    "# Retrieve the observations\n",
    "obs = envs.reset()  \n",
    "\n",
    "# Verify the keys of the observation\n",
    "print(\"Available observations :\", obs[0].keys())  # Obs of environment 0\n",
    "OmegaConf.set_readonly(config, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709ef45-dbe3-40f8-a608-307f4dd162b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Configuration 2 : PointNav, Multiple Rooms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76428fa-776d-4212-924f-6893efca0d52",
   "metadata": {},
   "source": [
    "### PPO Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbfcb6a-256b-44a7-952e-1c82242ce4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_PPO_config():\n",
    "    config = get_config(\"pointnav/ppo_pointnav.yaml\")\n",
    "    OmegaConf.set_readonly(config, False)\n",
    "    config.habitat_baselines.tensorboard_dir = \"tb/PPO\"\n",
    "    config.habitat_baselines.num_updates = -1\n",
    "    config.habitat_baselines.num_environments = 2\n",
    "    config.habitat_baselines.verbose = True\n",
    "    config.habitat_baselines.num_checkpoints = -1\n",
    "    config.habitat_baselines.checkpoint_interval = 1000000\n",
    "    config.habitat_baselines.total_num_steps = 150 * 1000\n",
    "    config.habitat.dataset.data_path=\"data/datasets/pointnav/multiple_room/v0/{split}/Adrian.json.gz\"\n",
    "    OmegaConf.set_readonly(config, True)\n",
    "\n",
    "    return config\n",
    "\n",
    "config = build_PPO_config()  # Build the config for PPO\n",
    "# Set randomness\n",
    "random.seed(config.habitat.seed)\n",
    "np.random.seed(config.habitat.seed)\n",
    "torch.manual_seed(config.habitat.seed)\n",
    "if (\n",
    "    config.habitat_baselines.force_torch_single_threaded\n",
    "    and torch.cuda.is_available()\n",
    "):\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "os.environ[\"MAGNUM_LOG\"] = \"quiet\"\n",
    "os.environ[\"HABITAT_SIM_LOG\"] = \"quiet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98bfb68-1eaa-4e4c-aebc-83c2710195fe",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc6657-cf9f-43fd-a58d-2bcafd11a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the trainer and start training\n",
    "trainer = PPOTrainer(config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d9d1b-e0f9-4936-80cd-9a1097d4f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config_2_logs = parse_tensorboard(\"tb/PPO\", [\"metrics/spl\", \"reward\", \"learner/grad_norm\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a3e9e-09fc-44b8-b622-afbffe4bb077",
   "metadata": {},
   "source": [
    "## Perfomances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c438f-0967-414b-90f1-ad2a3e6094ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(6, 6))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.axes(axes[0])\n",
    "\n",
    "plt.plot(ppo_config_1_logs['metrics/spl'], label=\"PPO\")\n",
    "plt.plot(ppo_config_2_logs['metrics/spl'], label=\"PPO\")\n",
    "\n",
    "plt.title(\"SPL\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.legend()\n",
    "\n",
    "plt.axes(axes[1])\n",
    "\n",
    "plt.plot(ppo_config_1_logs['learner/grad_norm'], label=\"PPO\")\n",
    "plt.plot(ppo_config_2_logs['learner/grad_norm'], label=\"PPO\")\n",
    "\n",
    "plt.title(\"Gradient Norm\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e80a8d-3e18-4d16-aeac-ae23dd5c37cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
