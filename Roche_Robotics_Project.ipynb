{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a658b9-1daa-4417-8eba-fe2081277340",
   "metadata": {},
   "source": [
    "# Roche robotics poject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3b4cc-d58d-4e3c-bf03-d8d054d77e87",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a3bad3e-cf4a-4e23-b219-9fd43199532a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither `ifconfig` (`ifconfig -a`) nor `ip` (`ip address show`) commands are available, listing network interfaces is likely to fail\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from habitat_baselines.config.default import get_config\n",
    "from habitat_baselines.rl.ppo.ppo_trainer import PPOTrainer\n",
    "from habitat_baselines.common.construct_vector_env import construct_envs\n",
    "from habitat_baselines.config.default import get_config\n",
    "\n",
    "from pg.base_pg import BasePolicyGradient\n",
    "from pg.base_pg_trainer import BasePolicyGradientTrainer\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a56b7-dd0f-4cc6-8f7e-cfcb26a1f120",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065f79ce-1509-4716-8875-ad7cbaaafe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tensorboard(path, scalars, prefix=\"\"):\n",
    "    \"\"\"returns a dictionary of pandas dataframes for each requested scalar\"\"\"\n",
    "    ea = event_accumulator.EventAccumulator(\n",
    "        path,\n",
    "        size_guidance={event_accumulator.SCALARS: 0},\n",
    "    )\n",
    "    _ = ea.Reload()\n",
    "    df = pd.DataFrame(ea.Scalars(scalars[0]))\n",
    "    df[f'{scalars[0]}'] = df['value']\n",
    "    df.drop(columns=['value'], inplace=True)\n",
    "    for k in scalars:\n",
    "        df[k] = pd.DataFrame(ea.Scalars(k))['value']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c124f396-b892-47a7-9c7e-f3770d39ef20",
   "metadata": {},
   "source": [
    "## Configuration 1 : PointNav, Empty Room"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f6688-fa79-41ed-8f4f-2df12ca5e617",
   "metadata": {},
   "source": [
    "### PPO Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b498bcaf-a121-4670-adc8-c5c4933d3657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_PPO_config():    \n",
    "    config = get_config(\"pointnav/ppo_pointnav.yaml\")\n",
    "    OmegaConf.set_readonly(config, False)\n",
    "    config.habitat_baselines.tensorboard_dir = \"tb/PPO\"\n",
    "    config.habitat_baselines.checkpoint_folder = \"data/PPO2_checkpoints\"\n",
    "    config.habitat_baselines.num_updates = -1\n",
    "    config.habitat_baselines.num_environments = 2\n",
    "    config.habitat_baselines.verbose = True\n",
    "    config.habitat_baselines.num_checkpoints = -1\n",
    "    config.habitat_baselines.checkpoint_interval = 1000000\n",
    "    config.habitat_baselines.total_num_steps = 150 * 1000\n",
    "    config.habitat.dataset.data_path = \"data/datasets/pointnav/simple_room/v0/{split}/empty_room.json.gz\"\n",
    "    config.habitat_baselines.load_resume_state_config = False\n",
    "    #config.habitat.simulator.scene = \"data/scene_datasets/simple_room/empty_room.glb\"\n",
    "\n",
    "    OmegaConf.set_readonly(config, True)\n",
    "\n",
    "    return config\n",
    "\n",
    "config = build_PPO_config()\n",
    "om_config = OmegaConf.to_yaml(config)\n",
    "#print(om_config)\n",
    "      \n",
    "# Set randomness\n",
    "random.seed(config.habitat.seed)\n",
    "np.random.seed(config.habitat.seed)\n",
    "torch.manual_seed(config.habitat.seed)\n",
    "if (\n",
    "    config.habitat_baselines.force_torch_single_threaded\n",
    "    and torch.cuda.is_available()\n",
    "):\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "os.environ[\"MAGNUM_LOG\"] = \"quiet\"\n",
    "os.environ[\"HABITAT_SIM_LOG\"] = \"quiet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08b884-0399-481d-ac6b-93f183ae0c68",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6b8d8e5-be85-4c09-8859-4e1062d101d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 09:45:06,434 Loading resume state: data/PPO2_checkpoints/.habitat-resume-state.pth\n",
      "2025-05-02 09:45:06,521 config: habitat:\n",
      "  seed: 100\n",
      "  env_task: GymHabitatEnv\n",
      "  env_task_gym_dependencies: []\n",
      "  env_task_gym_id: ''\n",
      "  environment:\n",
      "    max_episode_steps: 500\n",
      "    max_episode_seconds: 10000000\n",
      "    iterator_options:\n",
      "      cycle: true\n",
      "      shuffle: true\n",
      "      group_by_scene: true\n",
      "      num_episode_sample: -1\n",
      "      max_scene_repeat_episodes: -1\n",
      "      max_scene_repeat_steps: 10000\n",
      "      step_repetition_range: 0.2\n",
      "  simulator:\n",
      "    type: Sim-v0\n",
      "    action_space_config: v0\n",
      "    action_space_config_arguments: {}\n",
      "    forward_step_size: 0.25\n",
      "    create_renderer: false\n",
      "    requires_textures: true\n",
      "    auto_sleep: false\n",
      "    step_physics: true\n",
      "    concur_render: false\n",
      "    needs_markers: true\n",
      "    update_robot: true\n",
      "    scene: data/scene_datasets/habitat-test-scenes/van-gogh-room.glb\n",
      "    scene_dataset: default\n",
      "    additional_object_paths: []\n",
      "    seed: ${habitat.seed}\n",
      "    turn_angle: 10\n",
      "    tilt_angle: 15\n",
      "    default_agent_id: 0\n",
      "    debug_render: false\n",
      "    debug_render_robot: false\n",
      "    kinematic_mode: false\n",
      "    debug_render_goal: true\n",
      "    robot_joint_start_noise: 0.0\n",
      "    ctrl_freq: 120.0\n",
      "    ac_freq_ratio: 4\n",
      "    load_objs: false\n",
      "    hold_thresh: 0.15\n",
      "    grasp_impulse: 10000.0\n",
      "    agents:\n",
      "      main_agent:\n",
      "        height: 1.5\n",
      "        radius: 0.1\n",
      "        sim_sensors:\n",
      "          rgb_sensor:\n",
      "            type: HabitatSimRGBSensor\n",
      "            height: 256\n",
      "            width: 256\n",
      "            position:\n",
      "            - 0.0\n",
      "            - 1.25\n",
      "            - 0.0\n",
      "            orientation:\n",
      "            - 0.0\n",
      "            - 0.0\n",
      "            - 0.0\n",
      "            hfov: 90\n",
      "            sensor_subtype: PINHOLE\n",
      "            noise_model: None\n",
      "            noise_model_kwargs: {}\n",
      "          depth_sensor:\n",
      "            type: HabitatSimDepthSensor\n",
      "            height: 256\n",
      "            width: 256\n",
      "            position:\n",
      "            - 0.0\n",
      "            - 1.25\n",
      "            - 0.0\n",
      "            orientation:\n",
      "            - 0.0\n",
      "            - 0.0\n",
      "            - 0.0\n",
      "            hfov: 90\n",
      "            sensor_subtype: PINHOLE\n",
      "            noise_model: None\n",
      "            noise_model_kwargs: {}\n",
      "            min_depth: 0.0\n",
      "            max_depth: 10.0\n",
      "            normalize_depth: true\n",
      "        is_set_start_state: false\n",
      "        start_position:\n",
      "        - 0.0\n",
      "        - 0.0\n",
      "        - 0.0\n",
      "        start_rotation:\n",
      "        - 0.0\n",
      "        - 0.0\n",
      "        - 0.0\n",
      "        - 1.0\n",
      "        joint_start_noise: 0.1\n",
      "        robot_urdf: data/robots/hab_fetch/robots/hab_fetch.urdf\n",
      "        robot_type: FetchRobot\n",
      "        ik_arm_urdf: data/robots/hab_fetch/robots/fetch_onlyarm.urdf\n",
      "    agents_order:\n",
      "    - main_agent\n",
      "    habitat_sim_v0:\n",
      "      gpu_device_id: 0\n",
      "      gpu_gpu: false\n",
      "      allow_sliding: true\n",
      "      frustum_culling: true\n",
      "      enable_physics: false\n",
      "      physics_config_file: ./data/default.physics_config.json\n",
      "      leave_context_with_background_renderer: false\n",
      "      enable_gfx_replay_save: false\n",
      "    ep_info: null\n",
      "  task:\n",
      "    reward_measure: distance_to_goal_reward\n",
      "    success_measure: spl\n",
      "    success_reward: 2.5\n",
      "    slack_reward: -0.01\n",
      "    end_on_success: true\n",
      "    type: Nav-v0\n",
      "    lab_sensors: {}\n",
      "    measurements:\n",
      "      distance_to_goal:\n",
      "        type: DistanceToGoal\n",
      "        distance_to: POINT\n",
      "      success:\n",
      "        type: Success\n",
      "        success_distance: 0.2\n",
      "      spl:\n",
      "        type: SPL\n",
      "      distance_to_goal_reward:\n",
      "        type: DistanceToGoalReward\n",
      "    goal_sensor_uuid: pointgoal\n",
      "    count_obj_collisions: true\n",
      "    settle_steps: 5\n",
      "    constraint_violation_ends_episode: true\n",
      "    constraint_violation_drops_object: false\n",
      "    force_regenerate: false\n",
      "    should_save_to_cache: false\n",
      "    must_look_at_targ: true\n",
      "    object_in_hand_sample_prob: 0.167\n",
      "    min_start_distance: 3.0\n",
      "    render_target: true\n",
      "    physics_stability_steps: 1\n",
      "    num_spawn_attempts: 200\n",
      "    spawn_max_dists_to_obj: 2.0\n",
      "    base_angle_noise: 0.523599\n",
      "    ee_sample_factor: 0.2\n",
      "    ee_exclude_region: 0.0\n",
      "    base_noise: 0.05\n",
      "    spawn_region_scale: 0.2\n",
      "    joint_max_impulse: -1.0\n",
      "    desired_resting_position:\n",
      "    - 0.5\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    use_marker_t: true\n",
      "    cache_robot_init: false\n",
      "    success_state: 0.0\n",
      "    easy_init: false\n",
      "    should_enforce_target_within_reach: false\n",
      "    task_spec_base_path: habitat/task/rearrange/pddl/\n",
      "    task_spec: ''\n",
      "    pddl_domain_def: replica_cad\n",
      "    obj_succ_thresh: 0.3\n",
      "    enable_safe_drop: false\n",
      "    art_succ_thresh: 0.15\n",
      "    robot_at_thresh: 2.0\n",
      "    filter_nav_to_tasks: []\n",
      "    actions:\n",
      "      stop:\n",
      "        type: StopAction\n",
      "      move_forward:\n",
      "        type: MoveForwardAction\n",
      "      turn_left:\n",
      "        type: TurnLeftAction\n",
      "      turn_right:\n",
      "        type: TurnRightAction\n",
      "  dataset:\n",
      "    type: PointNav-v1\n",
      "    split: train\n",
      "    scenes_dir: data/scene_datasets\n",
      "    content_scenes:\n",
      "    - '*'\n",
      "    data_path: data/datasets/pointnav/simple_room/v0/{split}/empty_room.json.gz\n",
      "  gym:\n",
      "    auto_name: ''\n",
      "    obs_keys: null\n",
      "    action_keys: null\n",
      "    achieved_goal_keys: []\n",
      "    desired_goal_keys: []\n",
      "habitat_baselines:\n",
      "  trainer_name: ppo\n",
      "  torch_gpu_id: 0\n",
      "  video_render_views: []\n",
      "  tensorboard_dir: tb/PPO\n",
      "  writer_type: tb\n",
      "  video_dir: video_dir\n",
      "  video_fps: 10\n",
      "  test_episode_count: -1\n",
      "  eval_ckpt_path_dir: data/new_checkpoints\n",
      "  num_environments: 2\n",
      "  num_processes: -1\n",
      "  checkpoint_folder: data/PPO2_checkpoints\n",
      "  num_updates: -1\n",
      "  num_checkpoints: -1\n",
      "  checkpoint_interval: 1000000\n",
      "  total_num_steps: 150000.0\n",
      "  log_interval: 25\n",
      "  video_interval: 150\n",
      "  log_file: train.log\n",
      "  force_blind_policy: false\n",
      "  verbose: true\n",
      "  eval_keys_to_include_in_name: []\n",
      "  force_torch_single_threaded: true\n",
      "  wb:\n",
      "    project_name: ''\n",
      "    entity: ''\n",
      "    group: ''\n",
      "    run_name: ''\n",
      "    run_id: ''\n",
      "  load_resume_state_config: false\n",
      "  eval:\n",
      "    split: val\n",
      "    use_ckpt_config: true\n",
      "    should_load_ckpt: true\n",
      "    evals_per_ep: 1\n",
      "    video_option: []\n",
      "  profiling:\n",
      "    capture_start_step: -1\n",
      "    num_steps_to_capture: -1\n",
      "  launch_eval_afterwards: true\n",
      "  rl:\n",
      "    preemption:\n",
      "      append_slurm_job_id: false\n",
      "      save_resume_state_interval: 100\n",
      "      save_state_batch_only: false\n",
      "    policy:\n",
      "      name: PointNavResNetPolicy\n",
      "      action_distribution_type: categorical\n",
      "      action_dist:\n",
      "        use_log_std: true\n",
      "        use_softplus: false\n",
      "        std_init: ???\n",
      "        log_std_init: 0.0\n",
      "        use_std_param: false\n",
      "        clamp_std: true\n",
      "        min_std: 1.0e-06\n",
      "        max_std: 1\n",
      "        min_log_std: -5\n",
      "        max_log_std: 2\n",
      "        action_activation: tanh\n",
      "        scheduled_std: false\n",
      "      obs_transforms: {}\n",
      "      hierarchical_policy: ???\n",
      "    ppo:\n",
      "      clip_param: 0.2\n",
      "      ppo_epoch: 4\n",
      "      num_mini_batch: 2\n",
      "      value_loss_coef: 0.5\n",
      "      entropy_coef: 0.01\n",
      "      lr: 0.00025\n",
      "      eps: 1.0e-05\n",
      "      max_grad_norm: 0.5\n",
      "      num_steps: 128\n",
      "      use_gae: true\n",
      "      use_linear_lr_decay: true\n",
      "      use_linear_clip_decay: true\n",
      "      gamma: 0.99\n",
      "      tau: 0.95\n",
      "      reward_window_size: 50\n",
      "      use_normalized_advantage: false\n",
      "      hidden_size: 512\n",
      "      entropy_target_factor: 0.0\n",
      "      use_adaptive_entropy_pen: false\n",
      "      use_clipped_value_loss: true\n",
      "      use_double_buffered_sampler: false\n",
      "    ddppo:\n",
      "      sync_frac: 0.6\n",
      "      distrib_backend: GLOO\n",
      "      rnn_type: GRU\n",
      "      num_recurrent_layers: 1\n",
      "      backbone: resnet18\n",
      "      pretrained_weights: data/ddppo-models/gibson-2plus-resnet50.pth\n",
      "      pretrained: false\n",
      "      pretrained_encoder: false\n",
      "      train_encoder: true\n",
      "      reset_critic: true\n",
      "      force_distributed: false\n",
      "    ver:\n",
      "      variable_experience: true\n",
      "      num_inference_workers: 2\n",
      "      overlap_rollouts_and_learn: false\n",
      "    auxiliary_losses: {}\n",
      "\n",
      "2025-05-02 09:45:06,523 Initializing dataset PointNav-v1\n",
      "2025-05-02 09:45:07,022 Scenes are : empty_room\n",
      "2025-05-02 09:45:07,023 There are less scenes (1) than environments (2). Each environment will use all the scenes instead of using a subset.\n",
      "Neither `ifconfig` (`ifconfig -a`) nor `ip` (`ip address show`) commands are available, listing network interfaces is likely to fail\n",
      "Neither `ifconfig` (`ifconfig -a`) nor `ip` (`ip address show`) commands are available, listing network interfaces is likely to fail\n",
      "2025-05-02 09:45:18,270 Initializing dataset PointNav-v1\n",
      "2025-05-02 09:45:18,270 Initializing dataset PointNav-v1\n",
      "2025-05-02 09:45:18,558 initializing sim Sim-v0\n",
      "2025-05-02 09:45:18,559 initializing sim Sim-v0\n",
      "2025-05-02 09:45:23,161 Initializing task Nav-v0\n",
      "2025-05-02 09:45:23,170 Initializing task Nav-v0\n",
      "2025-05-02 09:45:23,297 agent number of parameters: 5772517\n",
      "2025-05-02 09:45:25,475 update: 500\tfps: 127.133\t\n",
      "2025-05-02 09:45:25,476 update: 500\tenv-time: 636.028s\tpth-time: 342.128s\tframes: 128000\n",
      "2025-05-02 09:45:25,476 Average window size: 50  distance_to_goal: 17.464  distance_to_goal_reward: 0.000  reward: 0.396  spl: 0.000  success: 0.000\n",
      "2025-05-02 09:46:02,493 update: 525\tfps: 128.756\t\n",
      "2025-05-02 09:46:02,494 update: 525\tenv-time: 655.452s\tpth-time: 359.135s\tframes: 134400\n",
      "2025-05-02 09:46:02,495 Average window size: 50  distance_to_goal: 17.783  distance_to_goal_reward: 0.000  reward: 0.340  spl: 0.000  success: 0.000\n",
      "2025-05-02 09:46:37,088 update: 550\tfps: 130.560\t\n",
      "2025-05-02 09:46:37,090 update: 550\tenv-time: 672.406s\tpth-time: 376.189s\tframes: 140800\n",
      "2025-05-02 09:46:37,090 Average window size: 50  distance_to_goal: 17.527  distance_to_goal_reward: 0.000  reward: 0.343  spl: 0.000  success: 0.000\n",
      "2025-05-02 09:47:12,019 update: 575\tfps: 132.212\t\n",
      "2025-05-02 09:47:12,021 update: 575\tenv-time: 689.705s\tpth-time: 393.236s\tframes: 147200\n",
      "2025-05-02 09:47:12,021 Average window size: 50  distance_to_goal: 17.284  distance_to_goal_reward: 0.000  reward: 0.388  spl: 0.000  success: 0.000\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Build the trainer and start training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m PPOTrainer(config)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/miniconda/envs/habitat/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/habitat-lab/habitat-baselines/habitat_baselines/rl/ppo/ppo_trainer.py:953\u001b[0m, in \u001b[0;36mPPOTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhabitat_baselines\u001b[38;5;241m.\u001b[39mlaunch_eval_afterwards:\n\u001b[0;32m--> 953\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_evaluation_after_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/habitat-lab/habitat-baselines/habitat_baselines/common/train_utils.py:399\u001b[0m, in \u001b[0;36mTrainUtils.launch_evaluation_after_training\u001b[0;34m(self, writer)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlaunch_evaluation_after_training\u001b[39m(\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    397\u001b[0m     writer: WeightsAndBiasesWriter,\n\u001b[1;32m    398\u001b[0m ):\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eval_monitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_evaluation_after_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/habitat-lab/habitat-baselines/habitat_baselines/common/train_utils.py:194\u001b[0m, in \u001b[0;36mEvalMonitor.launch_evaluation_after_training\u001b[0;34m(self, writer)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlaunch_evaluation_after_training\u001b[39m(\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    192\u001b[0m     writer: WeightsAndBiasesWriter,\n\u001b[1;32m    193\u001b[0m ):\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ckpt_fn \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ckpt_save_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mckpt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ckpt_fn:  \u001b[38;5;66;03m# Do not evaluate latest.pth\u001b[39;00m\n\u001b[1;32m    196\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoint'"
     ]
    }
   ],
   "source": [
    "# Build the trainer and start training\n",
    "trainer = PPOTrainer(config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d3893-6036-4d2e-b912-2f8fcc894904",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config_1_logs = parse_tensorboard(\"tb/PPO\", [\"metrics/spl\", \"reward\", \"learner/grad_norm\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0972ed1f-04c8-4c0b-92f7-7bf7d6a0e868",
   "metadata": {},
   "source": [
    "### Agent sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d9acc-263d-49ee-b4bb-a74afade703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = build_PPO_config()\n",
    "OmegaConf.set_readonly(config, False)\n",
    "\n",
    "# Creating the environment\n",
    "envs = construct_envs(config)\n",
    "\n",
    "# Retrieve the observations\n",
    "obs = envs.reset()  \n",
    "\n",
    "# Verify the keys of the observation\n",
    "print(\"Available observations :\", obs[0].keys())  # Obs of environment 0\n",
    "OmegaConf.set_readonly(config, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709ef45-dbe3-40f8-a608-307f4dd162b2",
   "metadata": {},
   "source": [
    "## Configuration 2 : PointNav, Multiple Rooms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76428fa-776d-4212-924f-6893efca0d52",
   "metadata": {},
   "source": [
    "### PPO Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbfcb6a-256b-44a7-952e-1c82242ce4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_PPO_config():\n",
    "    config = get_config(\"pointnav/ppo_pointnav.yaml\")\n",
    "    OmegaConf.set_readonly(config, False)\n",
    "    config.habitat_baselines.tensorboard_dir = \"tb/PPO\"\n",
    "    config.habitat_baselines.num_updates = -1\n",
    "    config.habitat_baselines.num_environments = 2\n",
    "    config.habitat_baselines.verbose = True\n",
    "    config.habitat_baselines.num_checkpoints = -1\n",
    "    config.habitat_baselines.checkpoint_interval = 1000000\n",
    "    config.habitat_baselines.total_num_steps = 150 * 1000\n",
    "    config.habitat.dataset.data_path=\"data/datasets/pointnav/multiple_room/v0/{split}/Adrian.json.gz\"\n",
    "    OmegaConf.set_readonly(config, True)\n",
    "\n",
    "    return config\n",
    "\n",
    "config = build_PPO_config()  # Build the config for PPO\n",
    "# Set randomness\n",
    "random.seed(config.habitat.seed)\n",
    "np.random.seed(config.habitat.seed)\n",
    "torch.manual_seed(config.habitat.seed)\n",
    "if (\n",
    "    config.habitat_baselines.force_torch_single_threaded\n",
    "    and torch.cuda.is_available()\n",
    "):\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "os.environ[\"MAGNUM_LOG\"] = \"quiet\"\n",
    "os.environ[\"HABITAT_SIM_LOG\"] = \"quiet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98bfb68-1eaa-4e4c-aebc-83c2710195fe",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc6657-cf9f-43fd-a58d-2bcafd11a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the trainer and start training\n",
    "trainer = PPOTrainer(config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d9d1b-e0f9-4936-80cd-9a1097d4f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config_2_logs = parse_tensorboard(\"tb/PPO\", [\"metrics/spl\", \"reward\", \"learner/grad_norm\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a3e9e-09fc-44b8-b622-afbffe4bb077",
   "metadata": {},
   "source": [
    "## Perfomances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c438f-0967-414b-90f1-ad2a3e6094ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(6, 6))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.axes(axes[0])\n",
    "\n",
    "plt.plot(ppo_config_1_logs['metrics/spl'], label=\"PPO\")\n",
    "plt.plot(ppo_config_2_logs['metrics/spl'], label=\"PPO\")\n",
    "\n",
    "plt.title(\"SPL\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.legend()\n",
    "\n",
    "plt.axes(axes[1])\n",
    "\n",
    "plt.plot(ppo_config_1_logs['learner/grad_norm'], label=\"PPO\")\n",
    "plt.plot(ppo_config_2_logs['learner/grad_norm'], label=\"PPO\")\n",
    "\n",
    "plt.title(\"Gradient Norm\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e80a8d-3e18-4d16-aeac-ae23dd5c37cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
